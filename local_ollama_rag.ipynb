{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d5cab6-515e-4dd6-95ae-6393f0c4435c",
   "metadata": {},
   "source": [
    "## Ingesting PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0e2f74-7c4b-4665-8d87-bc00656f31e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erand\\AppData\\Local\\Temp\\ipykernel_30276\\3632673901.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from IPython.display import display as Markdown\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104c0b18-1c06-41a1-a2ca-f9ee23f4f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"3dtbp_2024.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38119195-9c91-4e58-aa46-8a74244032af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 1 3 3 6 7 0 1 4 2 0 2 5 4 4 3 6 S E S C I / 9 0 1 1 0 1 : I\\n\\n.\\n\\n.\\n\\n.\\n\\nO D | E E E I\\n\\n4 2 0 2 © 0 0 1 3 $ / 4 2 / 4 - 6 3 0 4 - 5 1 3 3 - 8 - 9 7 9 | ) S E S C I (\\n\\n.\\n\\ns\\n\\nm e t s y S t r e p x E e b a n a t s u S n o e c n e r e f n o C\\n\\nl\\n\\ni\\n\\nl\\n\\na n o i t a n r e t n\\n\\nI\\n\\nh t 4 4 2 0 2\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\nComparative Analysis of Deep Learning Models for Early Skin Cancer Detection Using 3D Total Body Photography\\n\\n1st Keshavagari Smithin Reddy Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham Chennai, India smithinreddy4@gmail.com\\n\\n2nd Ramya Polaki Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham Chennai, India ramyapolaki6046@gmail.com\\n\\n3rd V Sulochana Anna Administrative Staff College, Chennai, India sulo62002@yahoo.com\\n\\n4th Gundala Pallavi Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham Chennai, India g pallavi@ch.students.amrita.edu\\n\\n5th Prasanna Kumar R Department of Computer Science and Engineering Amrita School of Computing,Amrita Vishwa Vidyapeetham Chennai, India r prasannakumar@ch.amrita.edu\\n\\nAbstract—Skin cancer presents a significant public health challenge, with early detection being crucial in mitigating its life-threatening implications. This study explores the application of deep learning models for early skin cancer detection using 3D Total Body Photography (TBP) images from the ISIC dataset. The dataset includes single-lesion crops from a large and diverse population, enabling comprehen- sive algorithm training. The study evaluates four models: EfficientNet-B0, CNN, ResNet50, and Inception V3. Among these, EfficientNet-B0 achieved the highest accuracy of 98%, underscoring the importance of model selection in skin cancer diagnostics. The results emphasize the potential of deep learning models in aiding early detection and reducing mortality from skin cancer.\\n\\nUnlike them, BCCs and SCCs do not metastasize easily but they still can cause high morbidity and disfigurement if neglected.Thus, accurate and timely diagnosis is essential for effective management and better outcomes in patients.\\n\\nThe difficult part of this task is separating benign le- sions—which are safe—from malignant ones, which need to be treated right once. Despite their similarity to ma- lignant illnesses, benign conditions such as non-invasive tumors, cysts, or moles do not spread or invade adjacent tissues [3]. Conversely, untreated malignant ones may spread to other parts of the body with life threatening consequences.\\n\\nKeywords— Skin cancer, 3D Total Body Photogra- phy (TBP), Single-lesion crops, EfficientNet-B0, CNN, ResNet50. Inception V3 , benign and malignant lesions,\\n\\nI. INTRODUCTION\\n\\nSkin cancer presents a formidable public health chal- lenge worldwide, affecting millions annually with its in- cidence steadily rising. Factors such as prolonged expo- sure to ultraviolet (UV) radiation, genetic predisposition, and environmental influences contribute significantly to its development. Among all cancers, skin cancer ranks among the most prevalent types, encompassing a spectrum of malignancies ranging from less aggressive basal cell carcinomas (BCCs) and squamous cell carcinomas (SCCs) to the more lethal melanomas[1].\\n\\nThe potential for rapid metastasis even though early de- tection and treatment are possible is one of the reasons why melanoma stands out as a severe type of skin cancer[2].\\n\\nPrevention of skin cancer requires proactive measures, such as reducing UV exposure through sunscreen use, wearing protective clothing and minimizing time spent in the sun during peak hours. Early detection is important for reducing mortality and improving treatment outcomes[4]. Traditional diagnostic methods involve visual inspection by dermatologists and histopathological analysis using biop- sies. Even though they are effective, these approaches can be subjective, time-consuming and subject to variability among practitioners[5].\\n\\nVisual inspection and biopsies are the traditional di- agnostic methods that may be subjective and also con- sume more time. Dermatological diagnostics have been transformed by recent advancements in transfer learning & deep learning techniques[6]. These are based on pre- trained models to improve on diagnostic accuracy while analyzing large datasets of dermatoscopic images together\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1275\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\nwith clinical data helping doctors make informed decisions [7].\\n\\napproach with Adam optimizer proved superior. This re- search highlights its potential for accurate automated clin- ical diagnosis, suggesting broader applications in disease classification[10].\\n\\nFig. 1: benign or malignant\\n\\nIn addition, imaging technology has evolved: for in- stance 3D Total Body Photography (3D-TBP) which has completely changed the face of skin cancer detection through providing complete high-resolution surface images of every part of the body. 3D-TBP enables dermatologists to monitor changes in skin lesions over time, facilitating early detection of suspicious growth patterns or changes in color, shape, or size that may indicate malignancy [8]. Integrating 3D-TBP with ML algorithms enhances diagnostic accuracy and empowers healthcare providers to deliver timely interventions, thereby improving patient outcomes in the management of skin cancer.\\n\\nThis is how this paper is organized: An overview of relevant research publications is given in Section II. In Section III, the study’s historical context, the original ideas, and the suggested model’s methodology are all thoroughly explained. After a careful analysis of the model, Section IV presents the experimental findings. In Section V, the paper concludes by summarizing the research’s conclusions and contributions.\\n\\nRoni Yoeli-Bik et al. assessed the diagnostic perfor- mance of three ultrasonography-based risk models for dif- ferentiating between benign and malignant ovarian tumors in a US cohort. These models were the International Ovar- ian Tumor Analysis (IOTA) Simple Rules, IOTA Assess- ment of Different Neoplasias in the Adnexa (ADNEX), and Ovarian-Adnexal Reporting and Data System (O-RADS). 511 patients with adnexal masses treated between 2017 and 2022 were included in the retrospective analysis, which was carried out at a single university medical center. According to ROC analysis, ADNEX achieved an area under the curve (AUC) of 0.96 and O-RADS of 0.92, indicating strong dis- criminative performance. These models showed excellent sensitivity and negative predictive values, indicating that they may be used in clinical practice to minimize needless procedures and improve patient care[11].\\n\\nIn their research, Ward Hendrix, Nils Hendrix, and col- leagues developed and evaluated a deep learning-based AI system designed to detect actionable benign nodules, small lung cancers, and pulmonary metastases in non-screening chest CT scans. Conducted across two Dutch hospitals, the study involved training the AI system on diverse datasets and validating its performance against panels of thoracic radiologists. Results showed that the AI system achieved high sensitivity rates for detecting these conditions, com- parable to or exceeding those of radiologists, albeit with a slightly higher false positive rate. The findings suggest that the AI system could effectively assist radiologists in detecting and managing pulmonary nodules in clinical practice, potentially enhancing diagnostic accuracy and patient care[12].\\n\\nII. RELATED WORKS\\n\\nto evaluate how well different ultrasound scores performed in terms of diagnosing benign and malignant adnexal tumors. Using techniques includ- ing subjective sonographer assessment, IOTA simple rules (SR), IOTA simple rules risk assessment (SRRA), O-RADS classification, and the ADNEX model (with and without CA125), they performed a retrospective research on 122 women who had chronic adnexal masses. According to the findings, the basic rules approach had the highest specificity (89.2%) and the ADNEX model with CA125 had the highest sensitivity (95.1%). Comparable to expert subjective judgments, all approaches demonstrated similar diagnostic performance, indicating that these ultrasound scores are useful in differentiating adnexal masses[9]. .\\n\\nPelayo et al. sought\\n\\nM. Dabir et al. attempted to distinguish between benign and malignant findings in [68 Ga]-FAPI PET/CT by using quantitative SUV measurements. In a retrospective analy- sis, lesion-to-background ratios (LBR) of FAPI uptake in benign vs malignant lesions, SUVmax, SUVmean, and 155 patients with a range of malignancies were evaluated. The study found that compared to malignant lesions, benign lesions showed much lower FAPI uptake. Using receiver operating characteristic (ROC) curve analysis, the optimal cutoff values were identified. SUVmax achieved 78.8% sensitivity, 85.1% specificity, 82.0% accuracy, and 0.89 AUC. This suggests the potential utility of [68 Ga]-FAPI PET/CT in oncology for precise diagnosis of benign from malignant disorders[13].\\n\\nMahati Munikoti Srikantamurthy et al. aimed to auto- mate histopathological breast cancer subtype classification using a hybrid CNN-LSTM model. This model, leveraging transfer learning on the BreakHis dataset, achieved 99% accuracy for binary (benign vs. malignant) and 92.5% for multi-class subtype classification. Compared to traditional CNN models like VGG-16 and ResNet50, the CNN-LSTM\\n\\nWei Fan et al. aimed to assess the diagnostic value of ar- tificial intelligence (AI) in distinguishing benign and malig- nant pulmonary nodules (PNs) using computed tomography (CT) density. They analyzed clinical data from 130 patients with confirmed PNs, comparing the performance of AI- based software against physician interpretation. The study found AI achieved a sensitivity of 94.69% in identifying PNs, surpassing radiologists’ sensitivity of 85.40%. Statis-\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1276\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\ntical analysis showed AI’s screening capacity significantly outperformed physician readings (p ¡ 0.05). AI suggested 214 PNs, of which 195 were pathologically confirmed as malignant, demonstrating robust diagnostic accuracy with an area under curve (AUC) of 0.798. The study concludes that integrating AI with CT imaging enhances the preci- sion of early lung carcinoma detection and supports more informed clinical management decisions[14].\\n\\nIlkay Yildiz Potter et al. aimed to enhance clinical decision-making in bone tumor diagnosis by developing an automated approach using computed tomography (CT) imaging and machine learning.[15] The research, spanning from March 2005 to October 2020, focused on segmenting and classifying bone tumors as benign or malignant based on a dataset of 84 femur CT scans with histologically confirmed lesions. Employing a deep learning architec- ture, the study achieved significant milestones: accurate segmentation of tumor regions with a 56% average Dice score and up to 80% accuracy on optimal image slices, along with robust classification performance. Despite the dataset’s imbalance between benign and malignant cases, the model demonstrated high specificity (75%) and sen- sitivity (79%), with an average accuracy of 77%. This approach marks a crucial advancement in leveraging AI for precise bone tumor analysis, potentially revolutionizing clinical workflows by providing reliable support for biopsy decisions and enhancing overall diagnostic accuracy[16].\\n\\nMohammed Rakeibul Hasan et al. addresses the urgent need for early detection of skin cancer, particularly fo- cusing on distinguishing between benign and malignant types using convolutional neural networks (CNNs). The research employs various CNN models, including VGG16, ResNet50, and several sequentially built models, to analyze a dataset comprising 6,594 images of benign and malignant skin lesions sourced from Kaggle. Hasan evaluates these models based on their classification accuracy, with VGG16 achieving the highest accuracy of 93.18%, followed by ResNet50 at 84.39% and SVM at 83.48%. By leveraging CNNs, Hasan’s research contributes significant insights into optimizing diagnostic processes, potentially reducing the mortality rate associated with skin cancer through early intervention strategies[17].\\n\\nAtheer Bassel et al. focus on advancing skin cancer detection through a hybrid deep learning approach, aiming to enhance classification accuracy between malignant and benign types, They explore the effectiveness of combining deep neural networks (DNNs) like Resnet50, Xception, and VGG16 with traditional classifiers such as SVM, NN, RF, KNN, and logistic regression. The research leverages a dataset of 1,800 benign and 1,497 malignant skin images from the ISIC archive, employing a Stacking CV (cross- validation) methodology across three levels of classifi- cation. Results highlight the Xception model’s superior performance, achieving an accuracy of 90.9%, outperform- ing other feature extraction methods like Resnet50 and VGG16. The study underscores the potential of hybrid deep learning approaches in refining skin cancer diagnosis, advo-\\n\\ncating for further enhancement through larger, more diverse datasets and incorporation of image metadata to bolster model accuracy and reliability in clinical settings[18].\\n\\nShunichi Jinnai et al. explores the use of deep learning, specifically FRCNN, to classify clinical images of pig- mented skin lesions, including melanoma and basal cell carcinoma. Their study aims to compare FRCNN’s perfor- mance with dermatologists in distinguishing benign from malignant lesions. Using 5,846 images, FRCNN achieved 86.2% accuracy in multi-class and 91.5% accuracy in binary (benign vs. malignant) classification., outperforming both board-certified dermatologists and trainees. This un- derscores FRCNN’s potential as a reliable tool for early and accurate skin cancer detection, suggesting its integration into clinical practice could improve prognosis and patient outcomes[19, 20].\\n\\nIII. MATERIALS AND METHODOLOGY\\n\\nA. Data Set and Data Preprocessing\\n\\nThe International Skin Imaging Collaboration (ISIC) has curated the SLICE-3D 2024 Challenge Dataset, which con- sists of JPEG images and comprehensive metadata taken from 3D Total Body Photography (3D-TBP) scans. These scans, which were taken at several international institutions employing Vectra WB360 technology, offer high-resolution lesion crops that are crucial for improving the identifica- tion of skin cancer. Extensive clinical data is provided with each image, including patient details like age and sex, specific anatomical site information, and diagnostic labels classifying lesions as benign or malignant. Lesion size measures, color properties in the LAB color space, asymmetry markers, and five-level diagnostic categories are among the other metadata aspects. The participants’ task is to predict the chance of malignancy for each image in the dataset, which supports the primary objective of discriminating between benign and malignant lesions.From a methodological perspective, the dataset helps apply latest machine learning approaches, such as deep learning and transfer learning, to maximize clinical decision-making and diagnostic accuracy. The ISIC dataset used for this study consists of 6,594 images, with a class distribution of 80% benign lesions and 20% malignant lesions. To address class imbalance, the dataset was augmented using techniques such as random rotations, flips, and brightness adjustments, ensuring a more diverse set of training examples\\n\\nIn our study on skin cancer classification, rigorous data preprocessing was conducted to ensure data integrity and model efficacy. The dataset, sourced from the ISIC 2024 Challenge, underwent initial filtering to balance class repre- sentation, maintaining a ratio of 1:20 between positive and negative cases. This step aimed to address class imbalance inherent in medical datasets, enhancing the model’s ability to generalize across different skin lesion types.\\n\\nTo mitigate potential biases and ensure robust model evaluation, Stratified Group K-Fold Cross Validation was employed. This technique stratifies data based on class labels while grouping samples by patient ID to prevent\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1277\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\nat deeper levels by using skip connections or shortcuts to bypass certain layers.ResNet-50 can categorize compli- cated pictures due to its depth as well as powerful learning abilities.\\n\\nInception v3: Inception v3 uses a multi-scale architecture with dif- ferent filter sizes within the same layer. This allows it to identify objects of different scales and aspect ratios; hence its ability to extract information from multiple scales at once. Inception v3 architecture has gained popularity due to its efficiency in increasing image classification speed and accuracy.\\n\\nFig. 2: 3D Total Body Photography\\n\\ndata leakage across folds. The dataset was divided into five folds, with each fold containing balanced distributions of positive and negative cases representative of the entire dataset.\\n\\nCustom dataset classes, namely ISICDataset for Train and ISICDataset, were implemented to facilitate data load- ing and augmentation. Augmentations such as resizing, ran- dom rotation, flipping, scaling, hue/saturation adjustments, and brightness/contrast modifications were applied using the Albumentations library. These transformations were crucial in diversifying the training examples presented to the model, thereby improving its robustness to variations in skin lesion appearances. Preprocessing included resizing images to 384x384, random rotation, horizontal flipping, and color normalization. These transformations were cru- cial in increasing the model’s robustness to variations in image quality. The model architecture, based on the EfficientNet backbone from the timm library pretrained on ImageNet, featured a customized GeM (Generalized Mean Pooling) layer for effective feature extraction. During train- ing, the model optimized parameters using Binary Cross Entropy Loss, implemented through the AdamW optimizer. A Cosine Annealing scheduler dynamically adjusted learn- ing rates over epochs, optimizing model convergence and performance.\\n\\nEfficientNet-B0: EfficientNet-B0 employs a compound scaling method, where the network’s depth, width, and resolution are scaled uniformly to balance computational cost and accuracy. This results in an efficient model that achieves high accuracy with fewer parameters.\\n\\nThe AdamW optimizer was used during training, with a learning rate scheduler (Cosine Annealing) to dynamically adjust the learning rate. This helped the model converge faster and achieve better performance by preventing over- fitting.\\n\\nOur proposed EfficientNet-B0 model performs better with fewer parameters by maintaining network depth, width, and resolution through compound scaling mecha- nism. A baseline model is used in this case whereby a set of fixed scaling coefficients are adopted so that all dimensions such as depth, width and resolution can be evenly scaled. While performance is optimized using a balanced scaling approach the computational efficiency of the model is kept constant.\\n\\nB. Proposed Architecture\\n\\nInception v3, ResNet-50 and Convolutional Neural Networks (CNN) models after data processing. It is important to note that each of these models contributes to the improved overall performance and robustness of the system.\\n\\nThe study employed EfficientNet-B0,\\n\\nConvolutional Neural Networks (CNNs): Convolutional neural networks (CNNs) are extremely necessary for any image recognition tasks since they use layers of convolution filters to capture spatial hierarchy in images. Such filters scan the input image for features such as edges, textures and shapes that help in correct classification of images and identification of objects.\\n\\nFig. 3: Proposed Model Architecture\\n\\nA Generalized Mean (GeM) pooling layer, a potent method for aggregating features in convolutional neural networks, is incorporated into the suggested model. The definition of the GeM pooling layer is:\\n\\nGeM(X) =\\n\\n(cid:32)\\n\\n1 |X|\\n\\n(cid:88)\\n\\nxp\\n\\n(cid:33) 1\\n\\np\\n\\nx∈X\\n\\nResNet-50: ResNet-50 solves the vanishing gradient problem by employing residual learning which enables us to train deeper networks. This model maintains high accuracy even\\n\\nwhere X is the set of feature maps, x represents indi- vidual feature map values, and p is a learnable parameter that adjusts the pooling behavior. The GeM pooling layer generalizes various pooling techniques and helps capture\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1278\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\nsignificant features at multiple scales. By generalizing diverse pooling techniques like max pooling (p → ∞) and average pooling (p = 1), this pooling method offers a versatile way to capture significant characteristics at various scales.\\n\\nThe loss function used in this study is Binary Cross-\\n\\nEntropy Loss, defined as:\\n\\nBCE Loss = −\\n\\n1 N\\n\\nN (cid:88)\\n\\n[yi log(pi) + (1 − yi)log(1 − pi)]\\n\\ni=1\\n\\nwhere yi probability.\\n\\nis the true label and pi\\n\\nis the predicted\\n\\nThe model’s capacity to differentiate between classes is assessed using the Area Under the Receiver Operating Characteristic Curve (AUROC), a performance metric that is also used in conjunction with the loss function.\\n\\nextent it could possibly differentiate between the benign and malignant samples.\\n\\nThese models’ assessment yielded positive results and distinctively affirmed the ability of the models to accu- rately categorize benign and malign cases of the data sets. Specifically, the results on the EfficientNet-B0 out- performed other models by achieving a 98.6% accuracy, highlighting its ability to capture detailed features through compound scaling. This superior performance suggests that EfficientNet-B0 is particularly suited for early-stage skin cancer detection. ResNet-50 known for its deep and flexible model showed how it was capable of recognizing and differentiating the complex patterns with an accuracy of 95.2%. Inception v3 has been proved to work with multiple spatial hierarchies, the result is the accuracy of 92.7% percent The proposed CNN model obtained an accuracy rate of 88.6% percent with the help of simple architecture.\\n\\nI: HYPERPARAMETERS\\n\\nTABLE PROPOSED MODEL\\n\\nFOR\\n\\nTHE\\n\\nHyperparameter Seed Epochs Image Size Train Batch Size Validation Batch Size Learning Rate Scheduler Minimum Learning Rate T max Weight Decay\\n\\nValue 42 100 384 32 64 0.0001 Cosine Annealing LR 0.000001 500 0.000001\\n\\nThe overall dataset was further divided into training and testing dataset to evaluate the performance of our suggested model. It was observed that training data took a biggest portion (80%) of the total data set and helped the model to learn more data. The last 20% was kept aside for the purpose of validation to see how well the model does extrapolate over different unseen variables/factors. The data set comprised of pictures which were labelled either as benign or malignant, representing non-cancerous and can- cerous cases respectively. Each and every picture was pre- trained and annotated in a way that was suitable in order for the model to able to actually differentiate between these two classes. The binary annotations were described with 0 representing benign cancer and 1 representing malignancy; this enabled the utilisation of the binary cross-entropy loss function in the training of the model because this function assists to boost the performance of a model in tasks related to binary classifications.\\n\\nFig. 4: Accuracy for all the models\\n\\nthat blends efficiency and performance, attained the highest accuracy of 98.6%. This remarkable outcome reveals the ability of the program to identify complex patterns that differentiate between benign and malignant cases. The precision and recall values for each of the performance measures are summarized in Table II.\\n\\nEfficientNet-B0, a model\\n\\nTABLE II: MODEL PERFORMANCE METRICS Accuracy 98.6% 95.2% 92.7% 88.6%\\n\\nModel EfficientNet-B0 ResNet-50 Inception v3 CNN\\n\\nPrecision 0.98 0.95 0.92 0.88\\n\\nRecall 0.98 0.95 0.92 0.88\\n\\nIV. RESULTS AND DISCUSSION\\n\\nBased on the test results and statistical data, it is possible to determine the effectiveness of the proposed system for differentiating between benign and malignant tumors. The four models that are used in this challenge are EfficientNet- B0, ResNet-50, Inception v3 and CNN baseline model results are discussed in this section. These models were then trained, validated, and tested using the set data that was selected with proper care to see after how great an\\n\\nAccuracy =\\n\\nTP + TN TP + TN + FP + FN\\n\\nPrecision =\\n\\nTP TP + FP\\n\\nRecall =\\n\\nTP TP + FN\\n\\nThe performance of the proposed models was assessed using accuracy, precision, recall, and AUROC metrics.\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1279\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\n[2] S. R. Waheed, S. M. Saadi, M. S. Mohd Rahim, F. H Najjar, M. M. N. Mohd “Melanoma Skin Adnan, on CNN Deep Cancer Classification of Journal Learning Algorithms,” Malaysian Fundamental 19, vol. no. 3, pp. 299–305, May 2023. [Online]. Available: https://mjfas.utm.my/index.php/mjfas/article/view/2900\\n\\nSuaib,\\n\\nand A. A. Salim, based\\n\\nand Applied\\n\\nSciences,\\n\\nFig. 5: Confusion matrix for all the models\\n\\nEach model was trained using the ISIC dataset and evalu- ated using stratified group k-fold cross-validation to en- sure a robust estimation of performance. In particular, EfficientNet-B0 outperformed other models due to its abil- ity to balance depth, width, and resolution through com- pound scaling. The evaluation focused on distinguishing malignant lesions from benign, which is critical in early- stage cancer detection.\\n\\nTo get a better understanding of the model’s perfor- mance, confusion matrices were created to include the detailed analysis of true positives TP, true negative TN, false positive FP, and false negative FN[21]. These matrices help when it comes to comparing the recall and precision of each of the models. As for the evaluation of TP, TN, FP, and FN for each model, the confused matrix depicted in Fig 5 provides deep insight. These matrices enable the assessment of recall and, in turn, precision of each of the models. The confusion matrices, precision, and recall values, with reference to (2) and (3), demonstrate EfficientNet-B0’s superior performance when gem pooling is employed.\\n\\nV. CONCLUSION\\n\\nThis study demonstrates the potential of deep learning models, particularly EfficientNet-B0, in early skin cancer detection using 3D TBP. The high accuracy achieved suggests that deep learning can significantly improve diag- nostic accuracy in dermatology. Future work will focus on real-world clinical validation and the exploration of larger datasets to further enhance the model’s robustness. Addi- tionally, efforts will be made to integrate metadata, such as patient demographics and lesion history, into the models to provide more personalized diagnostic recommendations.\\n\\nREFERENCES\\n\\n[1] M. Naqvi, S. Q. Gilani, T. Syed, O. Marques, and H.-C. Kim, “Skin Cancer Detection Using Deep Learning—A Review,” Diagnostics, vol. 13, no. 11, p. 1911, May 2023. [Online]. Available: https://www.mdpi.com/2075-4418/13/11/1911\\n\\n[3] A. Erickson, M. He, E. Berglund, M. Marklund, R. Mirzazadeh, N. Schultz, L. Kvastad, A. Andersson, L. Bergenstr˚ahle, J. Bergenstr˚ahle, L. Larsson, L. Alonso Galicia, A. Shamikh, E. Basmaci, T. D´ıaz De St˚ahl, T. Rajakumar, D. Doultsinos, K. Thrane, A. L. Ji, P. A. Khavari, F. Tarish, A. Tanoglidi, J. Maaskola, R. Colling, T. Mirtti, F. C. Hamdy, D. J. Woodcock, T. Helleday, I. G. Mills, A. D. Lamb, and J. Lundeberg, “Spatially resolved clonal copy number alterations in benign and malignant tissue,” Nature, vol. 608, no. 7922, pp. 360–367, Aug. 2022. [Online]. Available: https://www.nature.com/articles/s41586-022-05023-2 [4] O. T. Jones, R. N. Matin, M. Van Der Schaar, K. Prathivadi Bhayankaram, C. K. I. Ranmuthu, M. S. Islam, D. Behiyat, R. Boscott, N. Calanzani, J. Emery, H. C. Williams, and F. M. Walter, “Artificial intel- ligence and machine learning algorithms for early detection of skin cancer in community and primary care settings: a systematic review,” The Lancet Digital Health, vol. 4, no. 6, pp. e466–e476, Jun. 2022. [5] P. K. Rangarajan, B. M. Gurusamy, E. Rajasekar, S. Ippatapu Venkata, and S. Chereddy, “Retroactive data structure for protein–protein interaction in lung cancer using dijkstra algorithm,” Int. J. Inf. Technol., vol. 16, no. 2, pp. 1239–1251, Feb. 2024.\\n\\n[6] K. S. Reddy, M. Rithani, P. K. Rangarajan, and G. B. Mohan, “A Comparative Analysis: Enhancing Baby Cry Detection with Hybrid Deep Learning Techniques,” in 2023 International Conference on Next Generation Electronics Vellore, India: IEEE, Dec. 2023, pp. 1–6. [Online]. Available: https://ieeexplore.ieee.org/document/10421119/ [7] Y. Kang, H. Park, B. Smit, and J. Kim, “A multi-modal pre-training transformer for universal learning in metal–organic frameworks,” transfer Nature Machine 5, 3, pp. 309–318, Mar. 2023. [Online]. Available: https://www.nature.com/articles/s42256-023-00628-2 [8] S. E. Cerminara, P. Cheng, L. Kostner, S. Huber, M. Kunz, J.-T. Maul, J. S. B¨ohm, C. F. Dettwiler, A. Geser, C. Jakopovi´c, L. M. Stoffel, J. K. Pe- ter, M. Levesque, A. A. Navarini, and L. V. Maul, “Diagnostic performance of augmented intelligence with 2D and 3D total body photography and convo- lutional neural networks in a high-risk population for melanoma under real-world conditions: A new era of skin cancer screening?” European Journal of Cancer, vol. 190, p. 112954, Sep. 2023.\\n\\n(NEleX).\\n\\nIntelligence,\\n\\nvol.\\n\\nno.\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1280\\n\\nProceedings of the International Conference on Sustainable Expert Systems (ICSES-2024) IEEE Xplore Part Number: CFP24VS6-ART; ISBN: 979-8-3315-4036-4\\n\\n[9] M. Pelayo,\\n\\nI. Pelayo-Delgado, J. Sancho-Sauco, J. Sanchez-Zurdo, L. Abarca-Martinez, V. Corraliza- Gal´an, C. Martin-Gromaz, M. J. Pablos-Antona, J. Zurita-Calvo, and J. L. Alc´azar, “Comparison of Ultrasound Scores in Differentiating between Benign and Malignant Adnexal Masses,” Diagnostics, vol. 13, no. 7, p. 1307, Mar. 2023. [Online]. Available: https://www.mdpi.com/2075-4418/13/7/1307\\n\\n[10] M. M. Srikantamurthy, V. P. S. Rallabandi, D. B. Dudekula, S. Natarajan, and J. Park, “Classifica- tion of benign and malignant subtypes of breast cancer histopathology imaging using hybrid CNN- LSTM based transfer learning,” BMC Medical Imag- ing, vol. 23, no. 1, p. 19, Jan. 2023.\\n\\n[11] R. Yoeli-Bik, R. E. Longman, K. Wroblewski, M. Weigert, J. S. Abramowicz, and E. Lengyel, “Diagnostic Performance of Ultrasonography-Based Risk Models in Differentiating Between Benign and Malignant Ovarian Tumors in a US Cohort,” JAMA Network Open, vol. 6, no. 7, p. e2323289, Jul. 2023. Scholten, J. Trap-de Jong, S. Schalekamp, M. Mourits, M. Korst, M. Van Leuken, B. Van Ginneken, Jacobs, “Deep M. Prokop, M. Rutten, and C. learning and of detection the malignant pulmonary nodules in non-screening chest CT scans,” Communications Medicine, vol. 3, no. 1, p. 156, Oct. 2023. [Online]. Available: https://www.nature.com/articles/s43856-023-00388-5 [13] M. Dabir, E. Novruzov, K. Mattes-Gy¨orgy, M. S. A. Koerber, M. R¨ohrich, C. Kratochwil, J. Debus, U. Haberkorn, and F. L. Giesel, “Distinguishing Benign and Malignant Findings on [68 Ga]- FAPI SUV Measurements,” Molecular Imaging and Biology, vol. 25, no. 2, pp. 324–333, Apr. 2023. [Online]. Available: https://link.springer.com/10.1007/s11307- 022-01759-5\\n\\n[12] W. Hendrix, N. Hendrix, E. T.\\n\\nbenign\\n\\nfor\\n\\nC. Antke,\\n\\nBeu, K. Dendl,\\n\\nPET/CT Based\\n\\non Quantitative\\n\\n“Comparative M. Analysis of Skin Cancer (Benign vs. Malignant) Detection Using Convolutional Neural Networks,” Journal of Healthcare Engineering, vol. 2021, [Online]. Available: pp. https://www.hindawi.com/journals/jhe/2021/5895156/ [18] A. Bassel, A. B. Abdulkareem, Z. A. A. Alyasseri, N. S. Sani, and H. J. Mohammed, “Automatic Malignant and Benign Skin Cancer Classification Using a Hybrid Deep Learning Approach,” Diagnostics, vol. 12, no. 10, p. 2472, Oct. 2022. [Online]. Available: https://www.mdpi.com/2075- 4418/12/10/2472\\n\\nKaur,\\n\\nand\\n\\nA.\\n\\nZaguia,\\n\\n1–17, Dec.\\n\\n2021.\\n\\n[19] S. Jinnai, N. Yamazaki, Y. Hirano, Y. Sugawara, Y. Ohe, and R. Hamamoto, “The Development of a Skin Cancer Classification System for Pigmented Skin Lesions Using Deep Learning,” Biomolecules, vol. 10, no. 8, p. 1123, Jul. 2020. [Online]. Available: https://www.mdpi.com/2218-273X/10/8/1123\\n\\n[20] P. K. Rangarajan, K. Venkatraman, C. Jawahar, B. Harish, S. Bharathraj, and M. Kumar, “Attention- guided residual network for skin lesion classification using deep reinforcement learning,” 11 2023, pp. 1–7. [21] R. Polaki and R. Annamalai, “A Comparative Study of Hybrid Deep Learning Techniques for COVID-19 Detection based on Cough Sound International Conference Analysis,” on Computing, Communication, and Intelligent Systems (ICCCIS). Greater Noida, India: IEEE, [Online]. Available: Nov. 2023, pp. 478–485. https://ieeexplore.ieee.org/document/10425787/\\n\\nin\\n\\n2023\\n\\n[14] W. Fan, H. Liu, Y. Zhang, X. Chen, M. Huang, and B. Xu, artificial “Diagnostic intelligence based on computed tomography (CT) pulmonary density investigation,” PeerJ, nodules: vol. 12, p. e16577, Jan. 2024. [Online]. Available: https://peerj.com/articles/16577\\n\\nvalue\\n\\nof\\n\\nin a\\n\\nand malignant\\n\\nbenign retrospective\\n\\n[15] H.-G. Kim, D.-Y. Lee, S.-Y. Jeong, H. H. Choi, J.-H. Yoo, and J. Hong, “Machine learning-based method for prediction of virtual network function resource demands,” 06 2019, pp. 405–413.\\n\\n[16] I. Yildiz Potter, D. Yeritsyan, S. Mahar, J. Wu, A. Nazarian, A. Vaziri, and A. Vaziri, “Automated Bone Tumor Segmentation and Classification as Be- nign or Malignant Using Computed Tomographic Imaging,” Journal of Digital Imaging, vol. 36, no. 3, pp. 869–878, Jun. 2023.\\n\\n[17] M. R. Hasan, M. I. Fatemi, M. Monirujjaman Khan,\\n\\n979-8-3315-4036-4/24/$31.00 ©2024 IEEE Authorized licensed use limited to: Queensland University of Technology. Downloaded on January 28,2025 at 06:06:54 UTC from IEEE Xplore. Restrictions apply.\\n\\n1281'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview first page\n",
    "Markdown(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faacc1-be29-4d52-a46e-94f5b5b8e728",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5435cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED    \n",
      "minicpm-v:latest           1862d7d5fee5    5.5 GB    11 days ago    \n",
      "llava:latest               8dd30f6b0cb1    4.7 GB    2 weeks ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    2 weeks ago    \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    4 weeks ago    \n",
      "gemma2:latest              ff02c3702f32    5.4 GB    4 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dcf2cfe-a7aa-4ecf-85e3-f77b9e850514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Pull nomic-embed-text model from Ollama if you don't have it\n",
    "# !ollama pull nomic-embed-text\n",
    "# # List models again to confirm it's available\n",
    "# !ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014e862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First clean up any existing ChromaDB installations\n",
    "%pip uninstall -y chromadb\n",
    "%pip uninstall -y protobuf\n",
    "\n",
    "# 2. Install specific versions known to work together\n",
    "%pip install -q protobuf==3.20.3\n",
    "%pip install -q chromadb==0.4.22  # Using a stable older version\n",
    "%pip install -q langchain-ollama\n",
    "\n",
    "# 3. Set the environment variable\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a39856-0cc0-4ebe-8024-9db32455a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad040e2-3abe-4e23-abb9-951b223b9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ed9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tonykipkemboi/YouTube/Coding Projects/ollama_pdf_rag/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/tonykipkemboi/YouTube/Coding Projects/ollama_pdf_rag/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/tonykipkemboi/YouTube/Coding Projects/ollama_pdf_rag/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/tonykipkemboi/YouTube/Coding Projects/ollama_pdf_rag/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/tonykipkemboi/YouTube/Coding Projects/ollama_pdf_rag/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. First clean up any existing ChromaDB installations\n",
    "%pip uninstall -y chromadb\n",
    "%pip uninstall -y protobuf\n",
    "\n",
    "# 2. Install specific versions known to work together\n",
    "%pip install -q protobuf==3.20.3\n",
    "%pip install -q chromadb==0.4.22  # Using a stable older version\n",
    "%pip install -q langchain-ollama\n",
    "\n",
    "# 3. Set the environment variable\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# 4. Now reimport with the new versions\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 5. Try creating the vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eadf50-2f3d-4420-8858-94e9c1682ffa",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ec338c4-f282-462f-b0a0-c1899538eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f6c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED    \n",
      "minicpm-v:latest           1862d7d5fee5    5.5 GB    11 days ago    \n",
      "llava:latest               8dd30f6b0cb1    4.7 GB    2 weeks ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    2 weeks ago    \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    4 weeks ago    \n",
      "gemma2:latest              ff02c3702f32    5.4 GB    4 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d6ceeb-6883-4688-b923-e771c2b2cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c436d5cd-5dd0-448c-b5c0-6eddab879c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71e423dc-f632-46f8-9bec-d74cb268ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb1f308f-8472-4506-9517-d79b61d408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06c25c1d-d205-409e-90a2-179d0bd7c41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 5 pillars of global cooperation, as analyzed by the Global Cooperation Barometer, are:\\n\\n1. **Trade and Capital**: Promote global development and resilience through the presence of global economic flows that promote likely opportunities for these outcomes.\\n2. **Innovation and Technology**: Accelerate innovation and beneficial technological progress through the global sharing of underlying knowledge that contributes to these outcomes by fostering collaboration across global talent.\\n3. **Climate and Natural Capital**: Focus on addressing climate change and promoting sustainable development through cooperation and collective action.\\n4. **Health and Wellness**: Promote global health and well-being through cooperative efforts in areas such as disease prevention, health security, and access to healthcare.\\n5. **Peace and Security**: Foster international peace and security through cooperation and conflict resolution mechanisms that promote stability and predictability.\\n\\nThese 5 pillars were chosen because of their impact on global development and their explicit dependence on cooperative efforts among nations and economies.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are the 5 pillars of global cooperation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfe79f21-48aa-4820-aa9f-79f3d1a0a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
